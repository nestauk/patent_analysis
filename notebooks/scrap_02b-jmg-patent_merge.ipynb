{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patent data integration and EDA\n",
    "\n",
    "This is an exploratory analysis of PATSTAT application data involving GB-based inventors and applicants.\n",
    "\n",
    "For more information about PATSTAT data check [here](https://www.epo.org/searching-for-patents/business/patstat.html#tab-1) and for more informatin about patent analysis in general go [here](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/463319/The_Patents_Guide_2nd_edition.pdf) \n",
    "\n",
    "### Activities\n",
    "\n",
    "* Load data (currently saved as a pickled dict where every element is a dataframe with information about an application)\n",
    "* Integrate data into a smaller set of tables for analysis\n",
    "* Explore data: \n",
    "  * What are the variables in the data?\n",
    "  * What are the missing and present values?\n",
    "  * What do the data capture *legally*?\n",
    "* Carry out an initial exploratory analysis\n",
    "  * What are the activity trends?\n",
    "  * Who are the top patenters (organisations)\n",
    "  * What are the top patenters (sectors / places)\n",
    "* What can we find about AI?\n",
    "\n",
    "\n",
    "### Outputs\n",
    "\n",
    "* A data dictionary and cleaned dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data\n",
    "\n",
    "We have stored the data in a pickled file with a list of dictionaries containing various information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/raw/20_6_2019_patent_outputs.p','rb') as infile:\n",
    "    pdict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains 8 dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have a quick look inside\n",
    "\n",
    "#Loop over items in the dict\n",
    "for k,v in pdict.items():\n",
    "    \n",
    "    print(k.upper())\n",
    "    print(len(k)*'=')\n",
    "    print('\\n')\n",
    "    \n",
    "    print(f'number of observations: {len(v)}')\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "    print(v.head())\n",
    "    \n",
    "    print('\\n')\n",
    "        \n",
    "    print('Columns')\n",
    "    print('======')\n",
    "    \n",
    "    print(v.columns)\n",
    "    \n",
    "    \n",
    "    print('\\n \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for patent outputs\n",
    "\n",
    "# for k,v in pdict.items():\n",
    "    \n",
    "#     print(f'* **{k}**:')\n",
    "#     #print(f' * description:')\n",
    "#     print(f' * length:{len(v)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA of patent output contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person - applications\n",
    "\n",
    "The person application dataframe contains information about GB inventors or applications - they are the seed for our patent analysis in the mapping innovation in Scotland project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(pdict['person_appln'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A couple of questions\n",
    "\n",
    "* Which name and ID do we use?\n",
    "* What's up with all those missing addresses?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papp = pdict['person_appln']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papp['han_name'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papp['psn_name'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The han_name seems to be missing universities - they are not in Orbis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papp.loc[papp['psn_name']=='UNIVERSITY OF CAMBRIDGE']['han_name'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `psn_name` as this is the 'official' patstat standardised name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's up with the missing addresses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papp['person_address'].isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_address_lookup = {row['psn_name']:row['person_address'] for ind,row in papp.dropna(axis=0,subset=['person_address']).iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the names with missing addresses are in this lookup?\n",
    "\n",
    "names_missing_add = papp.loc[papp['person_address'].isna()]['psn_name']\n",
    "\n",
    "names_missing_add.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting - many of the orgs with missing addresses are 'big organisations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(names_missing_add)-set(person_address_lookup.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still 218K with missing addresses - they are not in the person name - address lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do organisations have a single address, and how do we interpret it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This groups the data by organisations and creates a list of addresses. Do we have multiple addresses per name or only one?\n",
    "\n",
    "grouped_addresses = papp.groupby('psn_name')['person_address'].apply(lambda x: set(list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok - so there seems to be a lot of duplication here. One way to manage this would be to focus on harmonised names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([len(x) for x in grouped_addresses]).value_counts()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of names with multiple addresses - we will need to allocate them at the patent level. Also need to decide what to do with missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_addresses.loc[[(len(x)>50) for x in grouped_addresses]][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The organisations with many addresses are big organisations or very common names. Do people with very common names have a single person id or many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_addresses['UNILEVER']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a mess! The addresses are totally unstandardised\n",
    "\n",
    "We can at least extract their postcodes using nslp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract postcodes using NSPL, the postcode lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load it\n",
    "#TODO - remove hardcoded path\n",
    "\n",
    "nspl = pd.read_csv('/Users/jmateosgarcia/Desktop/data/nspl/NSPL_FEB_2018_UK.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of lowercase postcodes. We will focus on the first three letters as this will speed up the analysis\n",
    "postcodes = list(set(nspl['pcds'].apply(lambda x: x.lower().split(' ')[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase the patent applications too\n",
    "papp['address_lower'] = papp['person_address'].apply(lambda x: x.lower().split(' ') if pd.isnull(x)==False else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now extract the postcodes from the lowercase addresses (if present)\n",
    "papp['uk_postcode'] = [set(x) & set(postcodes) if type(x)==list else np.nan for x in papp['address_lower']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And now we want to extract full postcodes for those people where we found the 3-digit ones\n",
    "# This is a faff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Store full postcodes here\n",
    "\n",
    "full_postcode_store = []\n",
    "\n",
    "#We will loop over rows\n",
    "\n",
    "for ind,row in papp.iterrows():\n",
    "    \n",
    "    #If the postcode is nan that means we append a nan to our store\n",
    "    \n",
    "    if type(row['uk_postcode'])!=set:\n",
    "        full_postcode_store.append(np.nan)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "    #If we have a postcode, we extract it together with the address\n",
    "\n",
    "        pc = list(row['uk_postcode'])\n",
    "        add = row['address_lower']\n",
    "\n",
    "    # There were addresses with no postcodes - empty set. In there was at least one we will try to extract the string after it\n",
    "\n",
    "        if (len(pc)>0):\n",
    "\n",
    "            #Index for the postcode. Note that this is assuming that we had a unique postcode per address\n",
    "            ind = add.index(pc[0])\n",
    "\n",
    "            #print(ind+1)\n",
    "            #print(len(add))\n",
    "            \n",
    "            #In some cases we have the first three digit of the postcode at the end of the address. In that case we append those.\n",
    "            if ind+1 < len(add):\n",
    "                \n",
    "                #Join the postcode with the string immediately after.\n",
    "                #Note that in some cases this might append non-postcode strings. These won't be matched later on.\n",
    "                \n",
    "                \n",
    "                out = ' '.join([pc[0],add[ind+1]])\n",
    "                full_postcode_store.append(out)\n",
    "\n",
    "            #If we didn't have a full postcode we append the three digits extracted before.\n",
    "            else:\n",
    "                full_postcode_store.append(pc[0])\n",
    "        \n",
    "        # If the set was empty, this means we have no postcode\n",
    "        else:\n",
    "            full_postcode_store.append(np.nan)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papp['uk_postcode_long'] = full_postcode_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge with TTWAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase the postcodes as before\n",
    "\n",
    "nspl['pcds_lower'] = nspl['pcds'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the merge\n",
    "\n",
    "papp_geo = pd.merge(papp,nspl[['pcds_lower','laua','ttwa']],left_on='uk_postcode_long',right_on='pcds_lower',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add TTWA names\n",
    "# TODO: remove hardcoded path\n",
    "\n",
    "#Load the lookup\n",
    "ttwa_names = pd.read_csv('/Users/jmateosgarcia/Desktop/data/nspl/Documents/TTWA names and codes UK as at 12_11 v5.txt',delimiter='\\t')\n",
    "\n",
    "#Create the dict (is there a better way to do this?)\n",
    "ttwa_names_lookup = {x['TTWA11CD']:x['TTWA11NM'] for ind,x in ttwa_names.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map\n",
    "papp_geo['ttwa_name'] = papp_geo['ttwa'].map(ttwa_names_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we go. Looking good\n",
    "papp_geo['ttwa_name'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do individuals with very common names have different ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papp.loc[papp['psn_name']=='BAKER, MATTHEW'][['person_id','person_name','psn_name','psn_id','han_name','han_id','person_address']].sort_values('han_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unclear what is the link between ids and person names. We will need to match at the patent application id level and decide what we do with missing addresses. Somehow allocate missing addresses randomly based on address distributions for persons with the same name (or id?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add flags for wheter a person is applicant or inventor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papp_geo['is_inventor'],papp_geo['is_applicant'] = [[x>0 for x in papp_geo[var]] for var in ['invt_seq_nr','applt_seq_nr']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute TTWAs where this is missing (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: create a table that we can merge with the patent applications later\n",
    "\n",
    "We will group various bits of information by the patent id (which becomes the index we will use for merging). They include:\n",
    "\n",
    "* Inventor (`invt_seq_nr` different from zero) names, ids addresses and TTWAs\n",
    "* Applicant (`applt_seq_nr` different from zero) names, ids, addresses and TTWAS\n",
    "\n",
    "To do this, I will create a simple function `make_person_metadata`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_person_metadata(df,metadata,name,application_id='appln_id'):\n",
    "    '''\n",
    "    This function creates patent application level metadata about the persons involved.\n",
    "    \n",
    "    In order to produce metadata about applicants and inventors we will filter the df beforehand using the invt_seq_nr and applt_seq_nr variables\n",
    "    \n",
    "    Arguments:\n",
    "        -df is the patent person df with the relevant information\n",
    "        -metadata is the list of variables that we want to aggreate for each patent\n",
    "        -name is the prefix we will use to label the data (eg inv, appl)\n",
    "        -application_id is the application identifier\n",
    "        \n",
    "    Output:\n",
    "        -A df where every row is a patent application and the columns contain the metadat\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Generate the metadata for each variable and output\n",
    "    out = pd.concat([df.groupby(application_id)[var].apply(lambda x: list(x)) for var in metadata],axis=1)\n",
    "    \n",
    "    out.rename(columns = {x:name+'_'+x for x in out.columns},inplace=True)\n",
    "    \n",
    "    return(out)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the metadata variables of interest\n",
    "meta_vars = ['psn_name','psn_id','psn_sector','person_address','uk_postcode_long','ttwa','ttwa_name']\n",
    "\n",
    "#This is a list with a df of 'person applicants' and a df of person inventors\n",
    "subset_dfs = [papp_geo.loc[papp_geo[var]==True] for var in ['is_applicant','is_inventor']]\n",
    "\n",
    "#This extracts the metadata for applicant and inventor metadata sets \n",
    "pat_person_meta = pd.concat([make_person_metadata(df,metadata=meta_vars,name=name) for df,name in zip(subset_dfs,\n",
    "                                                                                                        ['appl','inv'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_person_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - some of these patents have missing applicants or inventors because eg these might be based outside of the uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the missing addresses of inventor / applicant dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dfs[0]['person_address'].isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appln\n",
    "\n",
    "The `appln` df contains information about patent applications, such as their year and their 'family' (the invention they refer to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = pdict['appln']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A couple of things to explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the relation between filing year and publication year?\n",
    "\n",
    "100*np.mean(app['earliest_filing_year']<=app['earliest_publn_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is as expected - patents are filed with the patent office, after which they are published"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of patent families - do they tend to be in the same jurisdiction or different ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app['docdb_family_id'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check the jurisdictions for the patent with the biggest family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.loc[app['docdb_family_id']==9905751]['appln_auth'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications in multiple jurisdictions suggesting that a focus on families helps us to avoid double counting.\n",
    "\n",
    "Read an easy to understand explanation in [Wikipedia](https://en.m.wikipedia.org/wiki/Priority_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.loc[app['docdb_family_id']==9905751]['nb_citing_docdb_fam'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All patents in a family receive the same number of citations. Another reason to focus on citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: create an app_subset with variables of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vars = ['appln_id','appln_nr','ipr_type','granted', 'appln_auth','appln_filing_year','earliest_publn_year',\n",
    "          'docdb_family_id','inpadoc_family_id','nb_citing_docdb_fam']\n",
    "\n",
    "\n",
    "app_subset = app[my_vars].set_index('appln_id')\n",
    "\n",
    "app_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(app_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(app_subset.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appln_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abst = pdict['appln_abstr']\n",
    "\n",
    "abst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(abst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abst_length = pd.Series([len(x) for x in abst['appln_abstract']])\n",
    "\n",
    "abst_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all patents are in English. Some of them are incredibly long!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of curiosity: \n",
    "\n",
    "* How many of them mention finance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(['financ' in x for x in abst['appln_abstract']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* And how many mention machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(['machine learning' in x for x in abst['appln_abstract']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite low - let's see if we can match the ai patents later and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appln_techfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techfield = pdict['appln_techn_field']\n",
    "\n",
    "techfield.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(techfield)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each patent is allocated a set of technology fields (with weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## techn_field_ipc\n",
    "\n",
    "This is a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_lookup = pdict['tls901_techn_field_ipc']\n",
    "\n",
    "tf_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(tf_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match the techn fields with the previous field (so we can do some interpretable exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techfield_labelled = pd.merge(techfield,tf_lookup.drop_duplicates('techn_field'),left_on='techn_field_nr',right_on='techn_field_nr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techfield_labelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techfield_labelled.groupby('techn_field')['weight'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to group the fields by patent ids\n",
    "\n",
    "tf_meta_vars = ['weight','techn_field_nr','techn_field']\n",
    "\n",
    "#I use the same function that I defined before (it's quite generic!)\n",
    "tech_grouped = make_person_metadata(techfield_labelled,metadata=tf_meta_vars,application_id='appln_id',name='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tls902_ipc_nace2\n",
    "\n",
    "This is a lookup between ip codes and nace. Won't be very useful for us as we don't have the nace codes...yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipc_nace_lookup = pdict['tls902_ipc_nace2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipc_nace_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(ipc_nace_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUTS lookup (for completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_lookup = pdict['tls904_nuts']\n",
    "\n",
    "nuts_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(nuts_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine sources\n",
    "\n",
    "Here we will combine all the tables so far:\n",
    "\n",
    "* `app_subset` has the applications\n",
    "* `pat_person_meta` has the persons\n",
    "* `abstr` has the abstracts\n",
    "* `techfield_labelled` has the patent tech fields (with labels)\n",
    "\n",
    "The other dfs are not massively relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dfs = [app_subset,pat_person_meta,abst,\n",
    "         tech_grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,df in zip(['appl','person','abstract','field'],processed_dfs):\n",
    "    \n",
    "    print(name)\n",
    "    print('===')\n",
    "    \n",
    "    print(len(df))\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = pd.concat(processed_dfs,axis=1,join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat.reset_index(drop=False,inplace=True)\n",
    "\n",
    "print('|name|type|observations|')\n",
    "print('|----|----|----|')\n",
    "\n",
    "for c in pat.columns:\n",
    "    \n",
    "    print(f'|{c}|{type(pat[c].iloc[0])}|   |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there any ML patents in here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the IPO patents (downloaded from [here](https://www.gov.uk/government/publications/artificial-intelligence-a-worldwide-overview-of-ai-patents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_ids = list(pd.read_csv('../data/external/AI-raw-data.csv',header=None)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to match these patents with our data we need to create a new id that combines granting authority code and publication number \n",
    "pat['raw_ids'] = [x+y for x,y in zip(pat['appln_auth'],pat['appln_nr'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What's the overlap between both groups?\n",
    "uk_ai_pats = set(list(pat['raw_ids'])) & set(ml_ids)\n",
    "\n",
    "len(uk_ai_pats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1012 - not so bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat['is_ai_ipo'] = [x in uk_ai_pats for x in pat['raw_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat.to_csv(f'../data/processed/{today_str}_patent_table.csv',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
